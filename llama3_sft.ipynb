{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b132395f-78bf-48b7-9fe3-a4bdcd25a509",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip install mlflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69dbab7-6f73-4f0a-94c3-ea30c6ab534e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "from peft import LoraConfig, PeftModel, prepare_model_for_kbit_training\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "from trl import ORPOConfig, ORPOTrainer, setup_chat_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781e4195-2d3a-4d87-b920-8cfc3511f911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Downloading shards: 100%|██████████| 4/4 [01:39<00:00, 24.86s/it]\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:09<00:00,  2.29s/it]\n"
     ]
    }
   ],
   "source": [
    "# Model\n",
    "base_model = \"NousResearch/Meta-Llama-3-8B\"\n",
    "new_model = \"OrpoLlama-3-8B\"\n",
    "\n",
    "torch_dtype = torch.float16\n",
    "\n",
    "# QLoRA config\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch_dtype,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "\n",
    "# LoRA config\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=['up_proj', 'down_proj', 'gate_proj', 'k_proj', 'q_proj', 'v_proj', 'o_proj']\n",
    ")\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,\n",
    "                                         cache_dir=\"/mnt/artifacts/llama3-tokenizer-cache/\")\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=f\"/mnt/artifacts/llama3-model-cache/\"\n",
    "    # attn_implementation=attn_implementation\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "model = prepare_model_for_kbit_training(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d87ed9c-4d79-4935-8316-86a298e6221d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 2.92k/2.92k [00:00<00:00, 24.9MB/s]\n",
      "Downloading data: 100%|██████████| 115M/115M [00:00<00:00, 157MB/s]  \n",
      "Generating train split: 100%|██████████| 44245/44245 [00:00<00:00, 79088.74 examples/s]\n",
      "Map (num_proc=8): 100%|██████████| 1000/1000 [00:00<00:00, 3601.67 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"mlabonne/orpo-dpo-mix-40k\"\n",
    "dataset = load_dataset(dataset_name, split=\"all\")\n",
    "dataset = dataset.shuffle(seed=42).select(range(1000))\n",
    "\n",
    "def format_chat_template(row):\n",
    "    row[\"chosen\"] = tokenizer.apply_chat_template(row[\"chosen\"], tokenize=False)\n",
    "    row[\"rejected\"] = tokenizer.apply_chat_template(row[\"rejected\"], tokenize=False)\n",
    "    return row\n",
    "\n",
    "dataset = dataset.map(\n",
    "    format_chat_template,\n",
    "    num_proc= os.cpu_count(),\n",
    ")\n",
    "dataset = dataset.train_test_split(test_size=0.01)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a9599c6-406b-4e9b-95e6-a834b73e2a2e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/trl/trainer/orpo_trainer.py:247: UserWarning: When using DPODataCollatorWithPadding, you should set `remove_unused_columns=False` in your TrainingArguments we have set it for you, but you should do it yourself in the future.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:428: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='123' max='123' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [123/123 53:22, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Runtime</th>\n",
       "      <th>Samples Per Second</th>\n",
       "      <th>Steps Per Second</th>\n",
       "      <th>Rewards/chosen</th>\n",
       "      <th>Rewards/rejected</th>\n",
       "      <th>Rewards/accuracies</th>\n",
       "      <th>Rewards/margins</th>\n",
       "      <th>Logps/rejected</th>\n",
       "      <th>Logps/chosen</th>\n",
       "      <th>Logits/rejected</th>\n",
       "      <th>Logits/chosen</th>\n",
       "      <th>Nll Loss</th>\n",
       "      <th>Log Odds Ratio</th>\n",
       "      <th>Log Odds Chosen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.213900</td>\n",
       "      <td>3.384099</td>\n",
       "      <td>9.692700</td>\n",
       "      <td>1.032000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>-0.389522</td>\n",
       "      <td>-0.383038</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>-0.006484</td>\n",
       "      <td>-3.830378</td>\n",
       "      <td>-3.895217</td>\n",
       "      <td>-2.058413</td>\n",
       "      <td>-2.042024</td>\n",
       "      <td>3.294960</td>\n",
       "      <td>-0.891384</td>\n",
       "      <td>-0.074043</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.482300</td>\n",
       "      <td>2.733676</td>\n",
       "      <td>9.691800</td>\n",
       "      <td>1.032000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>-0.295852</td>\n",
       "      <td>-0.300485</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.004634</td>\n",
       "      <td>-3.004853</td>\n",
       "      <td>-2.958516</td>\n",
       "      <td>-2.071503</td>\n",
       "      <td>-2.052168</td>\n",
       "      <td>2.652839</td>\n",
       "      <td>-0.808365</td>\n",
       "      <td>0.031752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.286200</td>\n",
       "      <td>2.373255</td>\n",
       "      <td>9.696200</td>\n",
       "      <td>1.031000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>-0.245485</td>\n",
       "      <td>-0.254076</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.008591</td>\n",
       "      <td>-2.540760</td>\n",
       "      <td>-2.454846</td>\n",
       "      <td>-1.935581</td>\n",
       "      <td>-1.889193</td>\n",
       "      <td>2.295473</td>\n",
       "      <td>-0.777829</td>\n",
       "      <td>0.068980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.351600</td>\n",
       "      <td>2.210991</td>\n",
       "      <td>9.693300</td>\n",
       "      <td>1.032000</td>\n",
       "      <td>0.516000</td>\n",
       "      <td>-0.225141</td>\n",
       "      <td>-0.236281</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.011140</td>\n",
       "      <td>-2.362808</td>\n",
       "      <td>-2.251410</td>\n",
       "      <td>-1.886242</td>\n",
       "      <td>-1.803448</td>\n",
       "      <td>2.135420</td>\n",
       "      <td>-0.755712</td>\n",
       "      <td>0.095553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/peft/utils/save_and_load.py:168: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"MLFLOW_EXPERIMENT_NAME\"] = \"llama3_orpo\"\n",
    "# os.environ[\"MLFLOW_RUN_ID\"] = \"orpo_dpo_mix_40k_1\"\n",
    "\n",
    "\n",
    "orpo_args = ORPOConfig(\n",
    "    learning_rate=8e-6,\n",
    "    beta=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    max_length=1024,\n",
    "    max_prompt_length=512,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    num_train_epochs=1,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=0.2,\n",
    "    logging_steps=1,\n",
    "    warmup_steps=10,\n",
    "    report_to=\"mlflow\",\n",
    "    output_dir=\"/mnt/artifacts/results/\",\n",
    ")\n",
    "\n",
    "trainer = ORPOTrainer(\n",
    "    model=model,\n",
    "    args=orpo_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    peft_config=peft_config,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "trainer.train()\n",
    "trainer.save_model(f\"/mnt/artifacts/llama3_sft/{new_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f19fbfc2-9c86-4d8b-8b83-453d763c50dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:12<00:00,  3.13s/it]\n"
     ]
    }
   ],
   "source": [
    "# Flush memory\n",
    "del trainer, model\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Reload tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model,\n",
    "                                          cache_dir=\"/mnt/artifacts/llama3-tokenizer-cache/\"\n",
    "                                         )\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model,\n",
    "    low_cpu_mem_usage=True,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    cache_dir=\"/mnt/artifacts/llama3-tokenizer-cache/\"\n",
    ")\n",
    "model, tokenizer = setup_chat_format(model, tokenizer)\n",
    "\n",
    "# Merge adapter with base model\n",
    "model = PeftModel.from_pretrained(model, f\"/mnt/artifacts/llama3_sft/{new_model}\")\n",
    "model = model.merge_and_unload()\n",
    "model.save_pretrained(f\"/mnt/artifacts/llama3_sft/merged/{new_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52a78b6-7c13-4208-95d9-e561ec880227",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
